var tipuesearch = {"pages": [ {"title":"About","text":" About I am a Reader in Computing Science at the University of Glasgow. My research interests are compilers and runtime systems for heterogeneous architectures. I am particularly interested in FPGAs and acceleration of climate and weather simulations. My homepage at University of Glasgow Why I do Computing Science research My homepage at University of Glasgow Why I do Computing Science research Updated May 22, 2019 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"about/index.html"}, {"title":"wimvanderbauwhede.github.io Search resultswimvanderbauwhede.github.io Get Cooking!","text":" wimvanderbauwhede.github.io About wimvanderbauwhede.github.io About About Blog About Blog Wim Vanderbauwhede About Blog Terms Wim Vanderbauwhede About Blog Terms © 2016 wimvanderbauwhede.github.io Jekyll theme Skinny Bones","tags":"","url":"search/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/index.html"}, {"title":"Imagine we had better weather forecasts","text":" Imagine we had better weather forecasts The last few days have shown that current weather forecasts are at the same time very advanced and yet not good enough. We had little warning of the very large amount of snow that would cripple our infrastructure. Earlier and more accurate warnings could help to limit the damage of such events (estimated at £470m a day just from the travel disruption). Also, better long-range predictions about the probabilities of such events in the future could help with investment and planning of infrastructure and services: should councils invest in more snow ploughs; should rail operators invest in making the network more resilient to extreme cold weather; how can the emergency services be kept running in such extreme conditions, etc.? So why are our forecasts not better? One of the main reasons is that the resolution of the weather forecasting computer models is at the moment still quite coarse. For example the MetOffice forecasting model, which is considered amongst the best in the world, divides the UK in squares of 1.5 km at its highest resolution, i.e. the simulation produces a single averaged value anywhere within this 1.5 km x 1.5 km area. The time resolution for the shortest-term forecast is 50 seconds. In contrast, for accurate simulation of local weather, a resolution of hundred metres and a time step of about a second are required. This would require a supercomputer a thousand times more powerful than the one currently in use by the MetOffice. A key problem with supercomputers is that they consume a lot of power. The current MetOffice supercomputer consumes 2.7 MW of electricity. A supercomputer a thousand times more powerful would need 2.7GW which is more than twice as much as all the electricity produced by the UK’s largest nuclear power station, Hinkley Point B. To reduce the power consumption, new supercomputers have started using special hardware called accelerators. Already, both the fastest and most power-efficient supercomputers in the world use this technology. Unfortunately writing programs for such an accelerator-based supercomputer is very complicated. And existing programs can’t benefit from accelerators without major changes. Weather forecasting models are very large and complex, with around a million lines of code. Rewriting such a program is extremely difficult and time consuming. So what’s hindering progress? The holy grail is to develop a software technology that can automatically change legacy programs to make them suitable to the new, accelerator-based supercomputers. Many research groups, including my own, are working on such approaches. There is however a huge gap between a research proof-of-concept and a ready-for-use product and it takes considerable investment to bridge this gap. Unfortunately, a funding gap exists in this area: on the one hand, creating a product from a research proof-of-concept is not core research and can therefore not be funded by the Research Councils or through EU research funding. On the other hand, as there is no perceived commercial value in such a product (because the potential marked is very small), commercial funding is not an option. Imagine So imagine that the UK took a more joined-up view with investment to speed up the adoption of research. In the case of weather forecasting, this would help to minimize the impact on people and the economy of severe weather events like we experienced recently. It would be a thousandfold return on the investment. Updated March 04, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/better-forecasts/index.html"}, {"title":"Musings of an Accidental Computing Scientist","text":" Musings of an Accidental Computing Scientist April 27, 2019 Writing faster Perl code Sometimes your pure Perl code needs to be as fast as possible. What does it take? October 21, 2018 Why Europe needs hurricane contingency planning Hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. April 19, 2018 Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub A brief guide into hacking Pleroma, a federated microblogging server software. March 13, 2018 A little maths puzzle in two parts Of which the outcome is a way to construct a regular pentagon using compass and ruler. March 05, 2018 A strategy for articles/debugging For a long time it has been my contention that for a developer, more that programming, debugging should be treated as a core skill. March 04, 2018 Imagine we had better weather forecasts Current weather forecasts are at the same time very advanced and yet not good enough. Earlier and more accurate warnings could help to limit the damage of su... Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"index.html"}, {"title":"A strategy for articles/debugging","text":" A strategy for debugging For a long time it has been my contention that for a developer, more than programming, debugging should be treated as a core skill. A developer typically spends more time debugging code than writing code so it makes sense to try and optimize this process. Over the years I have developed a strategy for debugging. I see debugging as a generic, transferable skill that is applicable not only to coding but to any form of systems design. The mental models To debug a system we need a mental model, an understanding of the system in our mind. I believe this is the real cornerstone of debugging, and the common mistake is to spend too little time in constructing these models. The mental model should cover all aspects of the system that you need to understand. For example, if you want to understand why a program is slow, your mental model of the system should allow you to reason about the performance trade-offs. I distinguish three types of mental models, each of them corresponds to a different view of the system. The understanding of what the system does (or should do), how it should behave, is the behavioural model. A bug is observed through this model: the system behaves in a way that does not conform to the behavioural model. Usually (or at least if you’re lucky) the behavioural system model is codified in a specification. The operational system model is your understanding of how the system works. This model allows us to formulate hypotheses about why the system does not behave as expected. This is the most important mental system model, and part of the debugging process is actually improving and refining this model. In many cases, the operational model is actually your model of how a program in a given language is compiled/interpreted and executed on the hardware. This model starts from the syntax and semantics of the programming language, and includes a model for any API used in the code. As a trivial example, in Python, the keys in a dictionary are unordered, whereas the default in a C++ map is ordered. The closer you are to the bare metal, or the more you care about performance or memory footprint, the more details your mental model will have to include about the actual hardware, to the extent that for e.g. running code on FPGAs you even need to have a detailed mental model for the memory controllers. For debugging in higher-level languages, usually the model can be much more abstract, with a basic notion of memory management and code execution. The structural system model is the model of where we should look to trace and fix a bug. For software, this model is our understanding of the code structure. In general, the structure of software systems tends to be hierarchical and relatively loosely coupled. This means we only need to focus on a fraction of the codebase at a time. If this were not the case, debugging time would grow more than linearly with the code size. Fortunately for most systems it’s closer to logarithmic. The debugging activity Given the above mental system models, the activity of debugging is an iterative process involving several steps, and during the process we often jump between these steps. First, identify the bug. Is it really a bug or is your behavioural model incorrect, not specific enough or ambiguous? If necessary, adapt the model and re-iterate. Then there are essentially three stages in the process of finding the bug. We start by narrowing down through a process of exclusion: “This bug can’t be caused by X because of reason Y”. This process relies mostly on the operational system model, but sometimes also on the structural model, especially if you’re not 100% certain: “it is unlikely that the bug is in module X because of reason Y”. For example, it is unlikely that the cause of the bug is located in a standard library, compiler or interpreter. The chances that the bug is in your own code is much higher, so that possibility should be explored first. Once we cannot proceed any further through exclusion, we switch to the most interesting stage. We formulate a hypothesis “Let’s assume that the bug is caused by X” and then we use this as the basis for further investigation. The main difference between exclusion and formulating a hypothesis is that when we formulate a hypothesis, we don’t know if it is true or false, so we need to test it. With the exclusion process, we do know that our stated reason holds – or at least we have a high degree of confidence – so we don’t test it. Quite frequently our hypothesis will prove to be false, and then we have one fewer possible cause for the bug. Equally frequently, when our hypothesis proves to be false, this indicates that our operational model is incomplete. In that case we should formulate additional hypotheses to improve our mental model. I believe this is an important step that is often skipped because it seems to detract from the real task, i.e. finding the bug. But without an accurate operational model, it is much harder to find bugs, so the time spent in improving your system knowledge is always well spent. To test a hypothesis we can either use emulation or observation of the system behaviour. This requires the structural model to tell us where to look. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won’t find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. By emulation I mean that we mentally run part of a program using our operational model. In that case we assume that our operational model is accurate enough to produce the same result as the actual system. In general, this is a tricky approach to debugging because if our mental model is inaccurate we won’t find the bug. However, it is generally the approach taken when we have narrowed down the location of the bug sufficiently. We can observe the system behaviour through compiler or interpreter warnings, by using a debugger, or by making the code generate additional information. This requires a good structural model to guide us to the locations that we want to inspect using the debugger or where we want to add the code to generate the debugging information. Either way, the result should be some information that helps to test the hypothesis. Example scenarios Debugging code you wrote and understand, and whose use case is intimately familiar to you, should be the easiest type of debugging. However, the problem with this kind of code is often that, precisely because it is your own code and you have a very precise behavioural model, and of course a perfect structural model, you never bothered to create an accurate operational model of the code. This may sound strange because after all, if you wrote it, you should know how it works. But the reality is that we often perform very limited mental verification, esp. of corner cases, on our own code. Debugging someone else’s code is much harder because you typically lack all of the mental models. I often have to debug code written by my students, usually long after they have graduated. The main conclusion is that we should teach our students how to write maintainable code, i.e. code that makes it easy to understand the structural model. If the code is a Minimal, Complete, and Verifiable example MCVE then building your mental models is relatively easy because the code base should be small and self-contained. For such examples, the operational model is usually defined at the level of language semantics and standard library APIs. There is a nice detailed post about debugging small programs on Eric Lippert’s blog. The main challenge with debugging a truly huge codebase (millions of lines of code) is that you need to build mental models that cover the overall system, even if you are looking to debug a very specific aspect of the system behaviour. For example, some years ago I modified the Weather Research and Forecasting model to run on GPUs, and debugged the changes. This is a numerical weather simulator with a codebase of about two million lines of Fortran 90. It is very well architected and there is reasonably good documentation. The main challenge in this system was actually to understand the build system first, because a large amount of code is generated at build time. Apart from that, I had to learn how a weather simulator works at the level of the physics, and how the code was parallelised. I modified the part of the code known as the advection kernel, to make it work on GPUs. As expected, the changes were not first-time-right, and debugging GPU code is difficult because it is hard to observe what happens inside the GPU. Nevertheless, I followed essentially the approach outlined above. In this case, the original, unmodified code provided the reference behavioural model. I built the structural model through the process of working out which part of the code needed to be modified. So the difficulty was as usual with the operational model, and in this case the bugs mostly originated from the fact that the GPU code is essentially C, and the host code Fortran, and they have different views on arrays and argument passing. Conclusion Debugging is difficult and time consuming but a strategy based on behavioural, operational and structural mental models can make the process more efficient in a variety of scenarios. I would like to thank Ahmed Fasih for motivating me to write this article and suggesting the example scenarios. Updated March 05, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"debugging/index.html"}, {"title":"A little maths puzzle in two parts","text":" A little maths puzzle in two parts I like solving little maths puzzles, deriving known results using nothing more complicated than secondary school level trigonometry, algebra and maybe a little calculus. The one below is actually two puzzles, but both have to do with regular polygons. Part 1 Approximating π by the perimeter of a regular polygon Can one approximate $\\pi$ by the perimeter of regular polygon with increasing numbers of sides inscribed in a unit circle, so starting with a triangle, then a hexagon, a dodecagon,… ? You may say that is obvious: the more sides, the closer the polygon approximates a circle. But I still wanted to work out the proof. From Figure 1, clearly For a polygon with n sides, we have So we can express the lenght of the side of a polygon as a function of the number of sides as The perimeter of a regular polygon is obviously We know that (This is easily proven using de l’Hôpital’s rule ) Using $x=\\frac{\\pi}{n}$ it follows indeed that In other words the perimeter of successive polygons inscribed in a unit circle does approach $2\\pi$ – which of course comes hardly as a surprise. The following code implements this starting from a polygon with a given number of sides $n$ . The argument $h$ is half of the length of side, and $n_{max}$ is the number of iterations. What the algorithm does is recursively creating polygons with $n, 2n, 4n, 8n, … $ sides. The example starts from a triangle so $n=3$ and $h=\\sqrt(3)/2$. What I find interesting is that every prime polygon produces a different series but they all converge towards $\\pi$. Part 2 How to construct a pentagon using a compass and a ruler People have worked out how to construct a pentagon using only a compass and a ruler long ago. Nevertheless, I wanted to derive the construction from first principles. From Figure 2 we can write down some straightforward relationships between the length of a side of a pentagon (b from the previous part) and the angle of the arc, $\\frac{2\\pi}{5}$. Substitution of Eqs. $\\ref{eq:2.1}$ and $\\ref{eq:2.2}$ gives Now we consider the right triangle with hypothenuse q: Substitution of Eq. $\\ref{eq:2.3}$ in the RHS of Eq. $\\ref{eq:2.6}$ and refactoring gives: Substitution of Eq. $\\ref{eq:2.3}$ and Eq. $\\ref{eq:2.5}$ in the LHS of Eq. $\\ref{eq:2.7}$ and refactoring gives: Now we define $z=2y$ and rewrite Eq. $\\ref{eq:2.8}$ as: Which after more refactoring finally gives This is a third-order equation but fortunately there is an obvious root for $z=1$. After some factorization we obtain the remaining second-order equation: The roots of this equation are: This is actually a very famous equation and its positive root is known as the Golden ratio. Clearly y as defined is positive so $y=\\frac{z}{2}=\\frac{\\phi}{2}$ or From Eq. $\\ref{eq:2.13}$ we can express b in terms of y using Eqns $\\ref{eq:2.1}$, $\\ref{eq:2.2}$ and $\\ref{eq:2.3}$: And so we obtain the expression for the length of the side of a pentagon as The remaining question is then, how do we construct a line of length b using a rules and compass? We do this indirectly, by constructing a line of length y as shown in Figure 3. First, we construct a line of length 1/2. Then the hypothenuse of the right triangle with sides 1/2 and 1 has a lenght of $\\frac{\\sqrt{5}}{2}$. We add this to the 1/2 by drawing an arc of radius $\\frac{\\sqrt{5}}{2}$ using the compass. This way we get a line of length $\\phi = \\frac{1+\\sqrt{5}}{2}$. Dividing this into two gives y and through the way we constructed this, we immediately get b as well and so we can construct the pentagon using arcs of radius b. The equation we solved to obtain y (Eq. $\\ref{eq:2.10}$) is a third order equation, and its other positive root is $z=1$. This shows in a way the danger of transforming a geometry problem into algebra: only one of these roots, $z=\\phi$, corresponds to a solution of our geometric problem. But there is also a geometric interpretation for the root $z=1$. Substitution of $y=1/2$ in the equations for results in $q=\\sqrt{3}$ and $b=\\sqrt{3}$, in other words a regular triangle inscribed in the unit circle. Updated March 13, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/constructing-a-pentagon/index.html"}, {"title":"Why Europe needs hurricane contingency planning","text":" Why Europe needs hurricane contingency planning As a result of global warming, hurricanes will become common in Europe in the next few decades. I discuss the evidence, the impact and the need for proper contingency planning. Some terminology A tropical cyclone is a rapidly rotating storm system with a low-pressure centre, strong winds, and a spiral arrangement of thunderstorms that produce heavy rain. They are called “tropical” because they form almost exclusively over tropical or sub-tropical waters. “Cyclone” refers to their winds moving in a circle, rotating around a central clear eye. The winds blow counterclockwise in the Northern Hemisphere and clockwise in the Southern Hemisphere. For completeness, an anticyclone is a large-scale circulation of winds around a central region of high atmospheric pressure, clockwise in the Northern Hemisphere, counterclockwise in the Southern Hemisphere”. Anticyclones are not storm systems, and in Belgium, where I used to live, the famous1 Azores Anticyclone usually lead to nice weather. The terms “hurricane” and “typhoon” refer both to powerful tropical cyclones. A hurricane2 occurs in the Atlantic Ocean and northeastern Pacific Ocean, and a typhoon3 occurs in the northwestern Pacific Ocean4. The diameter of the hurricane is typically of the order of hundreds of kilometers. Last month’s hurricane Florence was over 600 km in diameter, and typhoon Trami about 500 km. A “tropical storm” is “tropical cyclone” that is less strong than a hurricane or typhoon. Strength and potential damage The Saffir-Simpson Hurricane Wind Scale is a scale from 1 to 5 based on a hurricane’s sustained wind speed, used to estimate potential property damage. Hurricanes reaching Category 3 and higher are considered major hurricanes because of their potential for significant loss of life and damage. However, Category 1 and 2 hurricanes are still much more dangerous than ordinary tropical storms, and require preventive measures. The amount of energy in a hurricane is very large. NOAA (US National Oceanic and Atmospheric Administration) notes that the energy released by an average hurricane “is equivalent to 200 times the world-wide electrical generating capacity”. The table below is adapted from NOAA page linked above. Note that this table uses sustained wind speeds. The gusts occuring during a tropical cyclone can be a lot stronger than this (typically about 30% stronger), and are usually what is quoted in the media. Global warming causes hurricanes to get stronger Slightly simplifying, hurricanes get their energy from the heat in the surface sea water. As the planet gets warmer, the sea surface gets warmer, which leads to stronger tropical cyclones. There is plenty evidence for this trend. For example, very recent work by my friend and colleague Prof. Takemi of the Disaster Prevention Research Institute (DPRI) of Kyoto University [1] used observation data of historical typhoons to reproduce them in simulation, and then simulated the effects of future warmer conditions. They conclude that both wind speed and precipitation would increase significantly. When I visited DPRI in September, a researcher explained me their latest simulations. With 2 degrees warming, wind speeds and rainfall during a typhoon could double. The recent Guardian article “Is climate change making hurricanes worse?” provides a good overview. Note that this trend refers to all tropical cyclones, not just these that made landfall. Hurricanes are coming to Europe Historically, none of the tropical storms in Europe in the last century except hurricane Vince in 2005 have been hurricanes, no matter how severe they might have seemed. However, this is about to change. As hurricanes get more powerful and last longer, the chance that they can reach Europe grows. Again, there is evidence for this. Already in 2013 Reindert Haarsma, Senior Scientist at the Royal Netherlands Meteorological Institute wrote the article “The future will bring hurricanes to Europe” which gives a good overview. The article is based on Haarsma’s scientific research [2]. Other researchers predict similar trends [3,4,5]. Impact and contingency planning Hurricanes do not only cause damage because of the strong winds. They also lead to flash floods because of the very heavy rainfall that they cause (often 30-50 cm/h), and because of the storm surges which are several meters in height (e.g. 5 m for hurricane Michael a few weeks ago). To put this into perspective, in December 2015 storm Desmond broke the United Kingdom’s 24-hour rainfall record with 34 cm of rainfall in 24 hours, and led to widespread flooding in the UK and Ireland. The only storm surge on record comparable to those caused by hurricanes was the notorious Great Storm of 1953. In Scotland, where storms are common, this was the worst storm5 in 500 years. Better flood defenses are therefore absolutely crucial to deal with future hurricanes. As a result of the 1953 disaster, much was done in the UK, the Netherlands and Belgium to strenghten flood defenses, but these focus on the North Sea. Similar works on the coasts facing the Atlantic will be necessary. Furthermore because of the increased damage, power outages and disruption of supplies will last much longer than for the storms we have now and therefore contingency plans will have to be put in place. A capability for accurate forecasting of hurricane trajectories is necessary for timely evacuation of people in the affected areas. The good news is that Europe can benefit from the extensive know-how developed for example in the US and Japan, both in predictions and in dealing with the effects of such severe weather events. [1] Kanada S, Takemi T, Kato M, Yamasaki S, Fudeyasu H, Tsuboki K, Arakawa O, Takayabu I. A multimodel intercomparison of an intense typhoon in future, warmer climates by four 5-km-mesh models. Journal of Climate. 2017 Aug;30(15):6017-36. [2] Haarsma RJ, Hazeleger W, Severijns C, De Vries H, Sterl A, Bintanja R, Van Oldenborgh GJ, van den Brink HW. More hurricanes to hit western Europe due to global warming. Geophysical Research Letters. 2013 May 16;40(9):1783-8. [3] Baker A, Hodges K, Schiemann R, Vidale PL. North Atlantic post-tropical cyclones in reanalysis datasets. In EGU General Assembly Conference Abstracts, 2018 Apr (Vol. 20, p. 14606). [4] Dekker MM, Haarsma RJ, de Vries H, Baatsen M, van Delden AJ. Characteristics and development of European cyclones with tropical origin in reanalysis data. Climate Dynamics. 2018 Jan 1;50(1-2):445-55. [5] Mousavi ME, Irish JL, Frey AE, Olivera F, Edge BL. Global warming and hurricanes: the potential impact of hurricane intensification and sea level rise on coastal flooding. Climatic Change. 2011 Feb 1;104(3-4):575-97. The banner image shows typhoon Halong approaching Japan in September 2014, © NASA Terra/MODIS 2014 It is so famous that a travel book shop in Brussels took it as its name. ↩ It is so famous that a travel book shop in Brussels took it as its name. ↩ The term “hurricane” derives from the Spanish word huracán, which in turn probably derives from the Taino (an indigenous people of the Caribbean) word hurakán “god of the storm”. ↩ The term “hurricane” derives from the Spanish word huracán, which in turn probably derives from the Taino (an indigenous people of the Caribbean) word hurakán “god of the storm”. ↩ In Japan they are called 台風 (taifuu) and are given numbers rather than names. ↩ In Japan they are called 台風 (taifuu) and are given numbers rather than names. ↩ In the northwestern Pacific, the term “super typhoon” is used for tropical cyclones with sustained winds exceeding 240 km/h. ↩ In the northwestern Pacific, the term “super typhoon” is used for tropical cyclones with sustained winds exceeding 240 km/h. ↩ This was a European windstorm, a type of extratropical cyclone, caused by different weather phenomena than hurricanes. There is evidence that this type of storms is also getting stronger [5]. ↩ This was a European windstorm, a type of extratropical cyclone, caused by different weather phenomena than hurricanes. There is evidence that this type of storms is also getting stronger [5]. ↩ Updated October 21, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/prepare-for-hurricanes/index.html"}, {"title":"Writing faster Perl code","text":" Writing faster Perl code As part of my research I have been developing a Fortran source-to-source compiler — in Perl. The compiler transforms legacy FORTRAN 77 scientific code into more modern Fortran 95. For the reasons to target FORTRAN 77, please read my paper. The compiler is written in Perl because it is available on any Linux-like system and can be executed without the need for a build toolchain. The compiler has no external dependencies at all, so that it is very simply to install and run. This is crucial because my target users are weather and climate scientists, not programmers or computer scientists. Python would have been a viable alternative but I personally prefer Perl. Perl performance as we know it An often-made argument is that if you want performance, you should not write your code in Perl. And it is of course true that compiled code will almost always be faster. However, often rewriting in a compiled language is not an option, so it is important to know how to get the best possible performance in pure Perl. The Perl documentation has perlperf which offers good advice and these tips provide some more detail. But for my needs I did not find the answers there, nor anywhere else. So I created some simple test cases to find out. I used Perl version 5.28, the most recent one, but the results should be quite similar for earlier versions. Testing some hunches Before going into the details on the performance bottleneck in my compiler, here are some results of performance comparisons that influenced design decisions for the compiler. The compiler is written in a functional style — I am after all a Haskell programmer — but performance matters more than functional purity. Fortran code essentially consists of a list of statements which can contain expressions, and the parser labels each of the statements once using a hashmap, ever the workhorse data structure in Perl. Every parsed line of code is stored as a pair with this hashmap (which I call $info): This means than in principle I can choose to match a pattern in $line using a regex or use one of the lables in $info. So I tested the performance of hash key testing versus regexp matching, using some genuine FORTRAN 77 code: Without the if-condidion, the loop takes 3.1 s on my laptop. The loop with the regexp match condition takes 10.1 s; with the hash key existence test it takes 5.6 s. So the actual condition evaluation takes 7 s for regexp and 2.5 s for hash key existence check. So testing hash keys is alsmost three times faster than simple regexp matching. I tested the cost of using higher-order functions for tree traversal. Basically, this is the choice between a generic traversal which takes an arbitrary function that operates on the tree nodes: or a custom traversal: For the case of the tree data structures in my compiler, the higher-order implementation takes twice as long as the custom traversal, so for performance this is not a good choice. Therefore I don’t use higher-order functions in the parser, but I do use them in the later refactoring passes. Finally I tested the cost of using map instead of a foreach-loop: The foreach-loop version takes 2.6 s, the map version 3.3 s, so the map is 25% slower. For reference, the index-based for-loop version takes 3.8 s and the C-style for-loop version 4.4 s — don’t do that! Because the map is slower, again I did not use it in the parser, and I implemented my own higher-order functions which use foreach-loops internally for the refactoring passes. Compiler bottleneck: expression parsing As the compiler grew in capabilities, it became noticeably slower. Perl has a great profiling tool, Devel::NYTProf, and I used it to identify the bottleneck. As you can see from the flame graph in the banner image, it turned out to be the expression parser. This part of the code was based on Math::Expression::Evaluator because it was convenient to reuse. But it was not built for performance, and also not to parse Fortran. So I finally bit the bullet and wrote my own. What I loosely call an expression parser is actually a combination of a lexer and a parser: it turns a string of source code into a tree-like datastructure which expresses the structure of the expression and the purpose of its constituents. For example if the expression is 2*v+1, the result of the expression parser will be a data structure which identifies the top-level expression as a sum of a multplication with the integer constant 1, and the multiplication of an integer constant 2 with a variable v. So how do we build a fast expression parser? It is not my intention to go into the computing science details, but instead to discuss the choices to be considered. Testing some more hunches First, the choice of the data structure matters. As we need a tree-like ordered data structure, it would have to either an object or a list. But objects in Perl are slow, so I use a nested list. As it happens, Math::Expression already uses nested lists. Using the parser from Math::Expression, the above expression would be turned into: This data structure is fine if you don’t need to do a lot of work on it. However, because every node is labeled with a string, testing against the node type is a string comparison. I did a quick test: On my laptop, the version with string comparison takes 5.3 s, the integer comparison 4.6 s. Without the if-statement, the code takes 3.1 s. In other words, the actual if with string comparison takes 2.2 s, with integer comparison 1.5 s. So doing string comparisons is 50% slower than doing integer comparisons. Therefore my data structure uses integer labels. Also, I label the constants so that I can have different labels for string, integer and real constants, and because in this way all nodes are arrays. This avoids having to test if a node is an array or a scalar, which is a slow operation. So the example becomes : Less readable, but faster and easier to extend. Then we have to decide how to parse the expression string. The traditional way to build an expression parser is using a Finite State Machine, consuming one character at a time (if needed with one or more characters look-ahead) and keeping track of the identified portion of the string. This is very fast in a language such as C but in Perl I was not too sure, because in Perl a character is actually a string of length one, so every test against a character is a string comparison. On the other hand, Perl has a famously efficient regular expression engine. So I created a little testbench to see which approach was faster: On my laptop, the FSM version takes 3.25 s, the regex version 1.45 s (mean over 10 runs), so the regexp version is twice as fast — the choice is clear. A faster expression parser With the choices of string parsing and data structure made, I focused on the structure of the overall algorithm. The basic approach is to loop trough a number of states and in every state perform a specific action. This is very simple because we use regular expressions to identify tokens, so most of the state transitions are implicit: The matching rules and operations are very simple (I use <pattern> and <integer> as placeholders for the actual values): prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } prefix operations: perl if ( $str=~s/^<pattern>// ) { $state=<integer>; } terms: perl if ( $str=~s/^(<pattern>)// ) { $expr_ast=[<integer>,$1]; } operators: perl $prev_lev=$lev; if ( $str=~s/^<pattern>// ) { $lev=<integer>; $op=<integer>; } Operators have precedence and associativity, and Fortran requires twelve precedence levels. In the “Append to AST” state, the parser uses $lev and $prev_lev to work out how the previously matched $expr_ast and $op should be appended to the @ast array. The prefix operations are handled by setting a state which is checked after term matching. The actual code is a bit more complicated because we need to parse array index expressions and function calls as well. This is done recursively during term matching; if a function call has multiple arguments, the parser is put into a new $state. So the end result is a minimally recursive parser, i.e. it only uses recursion when it is really necessary. This is because Perl is not efficient in doing recursive function calls (nor in fact for non-recursive ones). There is a lot of repetition of the patterns for matching terms and operators because if I would instead abstract the <pattern> and <integer> values by e.g. storing them in an array, the array accesses would considerably reduce the performance. I do store the precedence levels in an array because there are so many of them that the logic for appending terms to the AST would otherwise become very hard to read and update. Expression parser performance I tested the new expression parser on a set of 50 different expressions taken from a weather simulation code. The old expression parser takes 45 s to run this test a thousand times; the new expression parser takes only 2 s. In other words, the new parser is more than twenty times faster than the old one. It is also quite easy to maintain and adapt despite its minimal use of abstractions, and because it is Fortran-specific, the rest of the code has become a lot cleaner too. You can find the code in my GitHub repo. Summary Here is a summary of all optimisations I tested. The tests were run using Perl v5.28 on a MacBook Pro (late 2013), timings are averages over 5 runs and measured using time. Updated April 27, 2019 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/writing-faster-perl/index.html"}, {"title":"Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub","text":" Hacking the Pleroma: Elixir, Phoenix and a bit of ActivityPub Pleroma “is a microblogging server software that can federate (= exchange messages with) other servers that support the same federation standards (OStatus and ActivityPub). What that means is that you can host a server for yourself or your friends and stay in control of your online identity, but still exchange messages with people on larger servers. Pleroma will federate with all servers that implement either OStatus or ActivityPub, like GNU Social, Friendica, Hubzilla and Mastodon.” (stolen from Lain’s blog post). Recently I modified my Pleroma instance to support bot services: parse a posted message, take an action, post the result. To get there I had to learn Elixir, the language in which Pleroma is written, as well as Phoenix, the web framework Elixir uses, and a little bit about ActivityPub, the protocol for exchanging messages. What I want to explain here in particular is the architecture of Pleroma, so that you can hack it more easily, for fun or if you want to participate in the development. Elixir As Pleroma is written in Elixir you’ll need to learn that language to some extent. If you are familiar with Ruby (or Perl, for that matter) and with the idea of functional programming (everything is a function), then it is quite easy to learn and understand. The documentation and guides are very good. If you’ve never hear of functional programming, the main difference with e.g. Ruby or Java is that Elixir does not use an object-oriented programming model. Instead, there are functions that manipulate data structures and other functions. A particular consequence of the functional model is that there are no for- or while-loops. Instead, there are what is called higher-order functions which e.g. apply another function to a list. Elixir programs also make a lot more use of recursion. Another point about Elixir as a web programming language is that it is built on a system where processes communicate by passing messages to one another, and it is built in such a way that if a process dies it will normally be restarted automatically. This approach makes it very easy to offload work to separate worker processes etc. All this comes courtesy of Erlang, the language on which Elixir is built, with its powerfull OTP framework for building applications and its BEAM virtual machine, which manages the processes. Phoenix A lot of the groundwork of Pleroma is done by Phoenix, a very easy-to-use web server framework. Essentially, what happens is that the end user accesses the application using a specific url, typically via a web browser, and based on this url the application performs a number of actions, which in the end result in a change in the state of the application and usually in what is shown in the browser window. In Phoenix, there are five stages or components between the connection and the resulting action by the application: The endpoint is the boundary where all requests to your web application start. It is also the interface your application provides to the underlying web servers. Pleroma’s endpoint is web/endpoint.ex. If you look at the source you see several occurrences of plug(Plug...). Plug is a specification for composable modules in between web applications, and it is very heavily used in Pleroma. For example, to serve only specific static files/folders from priv/static: Another very nice feature of Phoenis is that you can edit your code while your server is running. It gets automatically recompiled and the affected processes are automatically restarted, courtesy of the Phoenix.CodeReloader: Routers are the main hubs of Phoenix applications. They match HTTP requests to controller actions, wire up real-time channel handlers, and define a series of pipeline transformations for scoping middleware to sets of routes. Pleroma’s router is web/router.ex. The key function in the router is the pipeline which lets you create pipelines of plugs. Other functions are scope, get, post, pipe_through, all of these let you match on the url and whether you are dealing with a get or post request, and define appropriate pipelines of actions. For example, federated ActivityPub requests handled as follows: where the pipe_through(:activitypub) call is used to insert a custom pipeline: Controllers are used to group common functionality in the same (pluggable) module. Pleroma makes heavy use of controllers: almost every request is handled by a specific controller for any given protocol, e.g. MastodonAPIController or ActivityPubController. This makes it easy to identify the files to work on if you need to make a change to the code for a given protocol. For example, the ActivityPub post requests in the Router are handled by inbox function in the ActivityPubController: Views are used to control the rendering of templates. You create a view module, a template and a set of assigns, which are basically key-value pairs. Pleroma uses views for “rendering” JSON objects. For example in web/activity_pub/activity_pub_controller.ex there are lines like Here, UserView.render is defined in web/activity_pub/views/user_view.ex for a number of different “*.json” strings. These are not really templates, they are simply used to pattern match on the function definitions. The more conventional usage to create HTML is also used, e.g. the template web/templates/mastodon_api/mastodon/index.html.eex is used in web/mastodon_api/mastodon_api_controller.ex via the view web/mastodon_api/views/mastodon_view.ex: ### Templates Templates are text files (typically html pages) with Elixir code to generate the specific values based on the assigns, included in <%= ... %>. For example, in Pleroma, the Mastodon front-end uses a template for the index.html file which has the code to show the name of the instance. Ecto Ecto is not a part of Phoenix, but it is an integral part of most web applications: Ecto is Elixir’s main library for working with databases. It provides the tools to interact with databases under a common API. Ecto is split into 4 main components: Ecto.Repo - repositories are wrappers around the data store. Via the repository, we can create, update, destroy and query existing entries. A repository needs an adapter and credentials to communicate to the database Pleroma uses the PostgresQL database. Ecto.Schema - schemas are used mainly to map tables into Elixir data (there are other use cases too). Ecto.Changeset - changesets provide a way for developers to filter and cast external parameters, as well as a mechanism to track and validate changes before they are applied to your data Ecto.Query - written in Elixir syntax, queries are used to retrieve information from the database. GenServer Because Elixir, like Erlang, uses a processes-with-message-passing paradigm, client-server relationships are so common that they have been abstracted as a behaviour, which in Elixir is a specification for composable modules which have to implement specified public functions (a bit like an interface in Java or typeclass in Haskell). If we look at the Federator.enqueue function, its implementation actually reduces to a single line: GenServer is an Elixir behaviour module for implementing the server of a client-server relation. The cast call sends an asynchronous request to the server (synchronous requests use call). The server behaviour is implemented using the handle_cast callback, which handles cast calls. In Pleroma.Federator, these are implemented in the same module as the enqueue function, hence the use of __MODULE__ rather than the hardcoded module name. Applications, Workers and Supervisors Elixir borrows the concept of a “supervision tree” from Erlang/OTP. AN application consists of a tree of processes than can either be supervisors or workers. The task of a supervisors is to ensure that the worker processes do their work, including distributing the work and restarting the worker processes when they die. Supervisors can supervise either worker or other supervisors, so you can build a supervision tree. Elixir provides an Application behaviour module and a Supervisor module to make this easy. The Application module requires a start() function as entry point. Typical code to create a supervision tree is where start_link() spawns the top process of the tree, and it spawns all the child processes in the list children. Pleroma uses a convenient but deprecated module called Supervisor.Spec which provides worker() and supervisor() functions, for example: Every worker has this own start_link function, e.g. in web/federator/federator.ex we find: This means that the Federator module borrows the start_link from the GenServer module. This is a very common way to create a worker. Mix Mix is the build tool for Elixir, and its main advantage is that the build scripts are also written in Elixir. Some key mix actions are provided by Phoenix, for example to build and run the final Pleroma application the action is mix phx.server. Hacking Pleroma After this brief tour of Elixir and Phoenix I want to give an example of adding simple bot functionality to Pleroma. See my fork of Pleroma for the code. My bot parses incoming messages for @pixelbot, extracts a list of pixel from the message, modifies a canvas with the new pixels and creates a PNG image of the result. It then posts a link to the PNG image. Because updating the canvas and creating the PNG image could be time-consuming, especially if the canvas were large, I put this functionality in a separate server module, and added this to the list of workers for the main Pleroma application: The bot takes the size of the canvas from my config.exs using the helper function get_canvas_size(). The id: PixelBot allows to access the worker by name. When the application starts, it launches the PixelBot worker (bots/pixelbot.ex). The worker calls its init() function (part of the GenServer behaviour) which loads the last canvas from a file. One of the protocols used for federation is ActivityPub. The specification is long and not so easy to read. However, for the purpose of hacking Pleroma it mainly helps to understand the structure of an ActivityPub action (in this case a post): In my case, In Pleroma this activity is linked to the Ecto repository Pleroma.Repo (repo.ex) in the module Pleroma.Activity (activity.ex), which defines a schema. The bot only supports ActivityPub. As we have seen above, in Pleroma incoming messages are handled by inbox function in the ActivityPubController (in web/activity_pub/activity_pub_controller.ex), so I put in a little hook there to detect if a message is for @pixelbot and has an actual message body (content): As you can see, the content of a message for @pixelbot is passed on to the PixelBot worker for processing using the GenServer.cast(Pleroma.Bots.PixelBot,content) call. The PixelBot worker parses the message to extract any pixels from it (bots/pixelbot/parse_messages.ex). If there are any, it updates the canvas (which is just a list of lists). It and writes the content to a file, and calls an external program to create the final image. Finally, the bot posts a status to the public timeline (bots/pixelbot/pixelbot_post_status.ex). The status contains the current time and a link to the latest canvas. The function pixelbot_post_status() creates the status and wraps it in the correct structure required by ActivityPub. It also gets the user object based on the nickname via Pleroma.User.get_cached_by_nickname(nickname). Like the activity, this user object is defined via a schema and linked to the Ecto repository (in user.ex). So user in the code below is a complicated object, not a url or nickname. Finally, the function calls ActivityPub.create() which creates the activity, and in this case that means it posts a status. Pleroma source tree This is only a part of the Pleroma source tree, it shows on the files mentioned above. Updated April 19, 2018 Wim Vanderbauwhede About Wim Vanderbauwhede About © 2019 Jekyll theme Skinny Bones","tags":"","url":"articles/hacking-pleroma.html"}, ]};